diff --git a/MTI_design_for_jaco_data.py b/MTI_design_for_jaco_data.py
index 02b81d4..3e75aaa 100644
--- a/MTI_design_for_jaco_data.py
+++ b/MTI_design_for_jaco_data.py
@@ -39,7 +39,7 @@ def dopplerFFT(f_name):
     dop_fft = np.fft.fftshift(np.fft.fft(range_fft, axis=1) / n, axes=1)
 
     #----- save dop in complex ---------
-    # np.save(save_dir + '/doppler_fft', dop_fft[:,:,:100,:])
+    np.save(f_name + '/doppler_fft_zero_pad_0_fir', dop_fft[:,:,:100,:])
 
     dop_fft = abs(dop_fft)
     return dop_fft
@@ -49,7 +49,7 @@ def rangeFFT(f_name):
     range_fft = np.fft.fft(raw_iq, axis=2) / n
 
     #------ save range in complex ------------
-    np.save(f_name + '/range_fft_zero_pad_0_iir', range_fft[:,:,:100,:])
+    # np.save(f_name + '/range_fft_zero_pad_0_iir', range_fft[:,:,:100,:])
     
     return range_fft
 
@@ -233,12 +233,12 @@ def main():
         # raw_iq = stoveMTI() # pre-processing using fir stove technique
         
         # FIR M=97 cut-off 20 hz
-        # raw_iq = np.reshape(raw_iq,(frame_number*chirp, adcSamples, TxRx))
-        # raw_iq = firMTI()
+        raw_iq = np.reshape(raw_iq,(frame_number*chirp, adcSamples, TxRx))
+        raw_iq = firMTI()
 
         # IIR M=12 cut-off 20 hz
-        raw_iq = np.reshape(raw_iq,(frame_number*chirp, adcSamples, TxRx))
-        raw_iq = iirMTI()
+        # raw_iq = np.reshape(raw_iq,(frame_number*chirp, adcSamples, TxRx))
+        # raw_iq = iirMTI()
 
         #### --------------------------------------------------------------------
 
@@ -254,7 +254,7 @@ def main():
         # plt.plot(freq, abs(range_fft[0,0,:,0])) 
         # plt.show()
 
-        # velocity_fft = dopplerFFT(f_name)
+        velocity_fft = dopplerFFT(f_name)
 
         # velocity_fft = velocity_fft[:,44:84,:,:]
         # print(velocity_fft.shape)
diff --git a/training_model/model_complex_to_complex.py b/training_model/model_complex_to_complex.py
index 930742f..add735a 100644
--- a/training_model/model_complex_to_complex.py
+++ b/training_model/model_complex_to_complex.py
@@ -30,10 +30,10 @@ def L2_fft_loss(output, label):
         L2_all_label = []
         output = output.permute(0,2,1)
         fft_out = torch.fft(output, 2)
-        modulus_fft = torch.sqrt(fft_out[:,:,0]**2 + fft_out[:,:,1]**2)
+        modulus_fft = torch.sqrt(fft_out[:,:,0]**2 - fft_out[:,:,1]**2)
         for i in range(label.size(1)):
             
-            l2_loss = torch.sqrt(label[:,i,:]**2 + modulus_fft**2)
+            l2_loss = torch.sqrt(label[:,i,:]**2 - modulus_fft**2)
             # print(l2_loss)
             l2_loss = torch.mean(l2_loss)
             
diff --git a/training_model/model_complex_to_complex_test.py b/training_model/model_complex_to_complex_test.py
index dd2163c..2601d7a 100644
--- a/training_model/model_complex_to_complex_test.py
+++ b/training_model/model_complex_to_complex_test.py
@@ -29,10 +29,10 @@ def L2_fft_loss(output, label):
         L2_all_label = []
         output = output.permute(0,2,1)
         fft_out = torch.fft(output, 2)
-        modulus_fft = torch.sqrt(fft_out[:,:,0]**2 + fft_out[:,:,1]**2)
+        modulus_fft = torch.sqrt(fft_out[:,:,0]**2 - fft_out[:,:,1]**2)
         for i in range(label.size(1)):
             
-            l2_loss = torch.sqrt(label[:,i,:]**2 + modulus_fft**2)
+            l2_loss = torch.sqrt(label[:,i,:]**2 - modulus_fft**2)
             # print(l2_loss)
             l2_loss = torch.mean(l2_loss)
             
diff --git a/training_model/model_modulus_to_modulus.py b/training_model/model_modulus_to_modulus.py
index 96fa5ae..f25c179 100644
--- a/training_model/model_modulus_to_modulus.py
+++ b/training_model/model_modulus_to_modulus.py
@@ -31,7 +31,7 @@ def L2_fft_loss(output, label):
         output = output.permute(0,2,1)
         # print(output.size())
         for i in range(label.size(1)):
-            l2_loss = torch.sqrt(label[:,i,:].pow(2) + output[:,:,0].pow(2))
+            l2_loss = torch.sqrt(label[:,i,:].pow(2) - output[:,:,0].pow(2))
             l2_loss = torch.mean(l2_loss)
             l2_loss_all.append(l2_loss)
         l2_loss_all = torch.tensor(l2_loss_all, requires_grad=True)
diff --git a/training_model/model_r_fir_pad_0.py b/training_model/model_r_fir_pad_0.py
index 1ae3703..71b75d6 100644
--- a/training_model/model_r_fir_pad_0.py
+++ b/training_model/model_r_fir_pad_0.py
@@ -13,22 +13,33 @@ import os
 import glob
 import re
 import warnings
+import argparse
 warnings.filterwarnings("ignore")
 
 folder_name = 'D:/data_signal_MTI/data_ball_move_39_real_imag_clean/p*'
+model_path = 'D:/signal_MTI/training_model/wandb/run-20200708_192200-25ly65p8/fir_6cov_1.pt'
+save_predict_path = 'D:/data_signal_MTI/data_ball_move_39_graph/'
+
+parser = argparse.ArgumentParser()
+parser.add_argument('-epochs', type=int, default=300)
+parser.add_argument('-batch_size', type=int, default=2000)
+parser.add_argument('-learning_rate', type=float, default= 0.001)
+parser.add_argument('-zero_padding', type=int, default=0)
+parser.add_argument('-test_batch_size', type=int, default= 2032)
+parser.add_argument('-loss_weight', type=int, default=3)
+parser.add_argument('-save_to_wandb', type=bool, default=False)
+parser.add_argument('-test_only', type=bool, default=False)
+parser.add_argument('-range_resolution', type=float, default=46.8410)
+args = parser.parse_args()
 
-epochs = 3000
-batch_size = 2000
-learning_rate = 0.001
-mm = 1e-3
-bin_resolution = 46.8410 ## millimeter = 4.6 cm
-padding = 0
 train_all = []
 test_all = []
 train_label_all = []
 test_label_all = []
 device = 'cuda' if cuda.is_available() else 'cpu'
-wandb.init(project="model_r_fir_pad_")
+
+if args.save_to_wandb:
+    wandb.init(project="model_r_fir_pad_")
 
 def L2_loss(output, label):
     m_r = meshgrid()
@@ -39,11 +50,11 @@ def L2_loss(output, label):
 def cartesian_to_spherical(label):
     y_offset = 110
     r = np.sqrt(label[:,0,0]**2 + (label[:,0,1] - y_offset)**2 + label[:,0,2]**2)
-    # print(r)
     return r
 
 def meshgrid():
-    m_r = torch.arange(0, bin_resolution*25, bin_resolution).to(device)
+    m_r = torch.arange(0, args.range_resolution*25, args.range_resolution).to(device)
+    # print("m_r", m_r.shape)
     return m_r
 
 def data_preparation(data_real, label):
@@ -72,12 +83,13 @@ class Model(nn.Module):
         self.encode_conv2 = nn.Conv1d(in_channels=4, out_channels=4, kernel_size=3, stride = 2, padding=1)
         self.encode_conv3 = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=3, stride = 1, padding=1)
         self.encode_conv4 = nn.Conv1d(in_channels=8, out_channels=8, kernel_size=3, stride = 2, padding=1)
-        self.encode_conv5 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, stride = 1, padding=1)
-        self.encode_conv6 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, stride = 2, padding=1)
+        # self.encode_conv5 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, stride = 1, padding=1)
+        # self.encode_conv6 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, stride = 2, padding=1)
 
-        self.fc1 = nn.Linear(in_features=4*16, out_features=200)
+        self.fc1 = nn.Linear(in_features=7*8, out_features=200)
         # self.fc2 = nn.Linear(in_features=200, out_features=100)
         self.fc3 = nn.Linear(in_features=200, out_features=25)
+        
 
 
     def forward(self, x):
@@ -86,8 +98,8 @@ class Model(nn.Module):
         x = F.leaky_relu(self.encode_conv2(x))
         x = F.leaky_relu(self.encode_conv3(x))
         x = F.leaky_relu(self.encode_conv4(x))
-        x = F.leaky_relu(self.encode_conv5(x))
-        x = F.leaky_relu(self.encode_conv6(x))
+        # x = F.leaky_relu(self.encode_conv5(x))
+        # x = F.leaky_relu(self.encode_conv6(x))
 
         x = x.view(x.size(0), -1)
         x = F.leaky_relu(self.fc1(x))
@@ -101,7 +113,11 @@ class Radar_train_Dataset(Dataset):
  
         data_real = np.load(real_part[0])
         label = np.load(label_file[0])
-        
+
+        #
+        data_real = data_real[5:]
+        label = label[5:]
+
         data_fft_modulus, label = data_preparation(data_real, label)
         
         train_all.extend(data_fft_modulus)
@@ -126,6 +142,10 @@ class Radar_test_Dataset(Dataset):
         data_real = np.load(real_part[0])
         label = np.load(label_file[0])
         
+        #
+        data_real = data_real[5:]
+        label = label[5:]
+
         data_fft_modulus, label = data_preparation(data_real, label)
         
         test_all.extend(data_fft_modulus)
@@ -145,11 +165,13 @@ class Radar_test_Dataset(Dataset):
 
 
 model = Model()
-wandb.watch(model)
+# wandb.watch(model)
+if args.test_only:
+    model.load_state_dict(torch.load(model_path))
 model.to(device)
 
 mse_loss = nn.MSELoss()
-optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
+optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)
 
 def train_function(train_loader):
     model.train()
@@ -161,7 +183,7 @@ def train_function(train_loader):
 
         optimizer.zero_grad()
         output = model(train_data)
-        loss, expect = L2_loss(output, train_labels)
+        loss, expect_r = L2_loss(output, train_labels)
         loss.backward()
         optimizer.step()
         avg_mini_train_loss.append(loss.item())
@@ -176,15 +198,16 @@ def test_function(test_loader):
         test_data = test_data.float()
         test_labels = test_labels.float()
         output = model(test_data)
-        loss, expect = L2_loss(output, test_labels)
-        # loss = loss*(1e-2)
+        loss, expect_r = L2_loss(output, test_labels)
+        
+        test_labels = test_labels.cpu().detach().numpy()
+        expect_r = expect_r.cpu().detach().numpy()
         avg_mini_test_loss.append(loss.item())
 
-    return np.mean(np.array(avg_mini_test_loss)), expect.cpu().detach().numpy(), test_labels.cpu().detach().numpy()
+    return np.mean(np.array(avg_mini_test_loss)), test_labels, expect_r 
     
 if __name__ == '__main__':
     
-    
     folder_name = glob.glob(folder_name)
     folder_name = natsort.natsorted(folder_name)
     count = 0
@@ -201,27 +224,37 @@ if __name__ == '__main__':
         else:
             train_data = Radar_train_Dataset(real_part= real_name, label_file=label_name)
             
-    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)
-    test_loader = DataLoader(dataset=test_data, batch_size=2032)
-
-    for epoch in range(epochs):
-        # print("======> epoch =", epoch)
-        train_loss = train_function(train_loader)
-        wandb.log({'Train_loss': train_loss}, step=epoch)
-        
-        # print(">>>>>> train_loss <<<<<<", train_loss)
-        if epoch%100 == 0:
-            test_loss, expect, expect_label = test_function(test_loader)
-            plt.plot(expect[:500])
-            plt.plot(expect_label[:500])
-            plt.ylabel('r distance')
-            plt.xlabel('number of test point')
-            print(">>>>>> test_loss <<<<<< epoch", epoch , test_loss)
-            wandb.log({'distance': plt}, step=epoch)
-            wandb.log({'Test_loss': test_loss}, step=epoch)
+    train_loader = DataLoader(dataset=train_data, batch_size=args.batch_size, shuffle=True)
+    test_loader = DataLoader(dataset=test_data, batch_size=args.test_batch_size)
+
+    if args.test_only:
+        test_loss, expect, expect_label = test_function(test_loader)
+        print(expect_label.shape)
+        np.save(save_predict_path + 'expect_r', expect_label)
+
+    else:
+        for epoch in range(args.epochs):
+            # print("======> epoch =", epoch)
+            train_loss = train_function(train_loader)
+            
+            if args.save_to_wandb:
+                wandb.log({'Train_loss': train_loss}, step=epoch)
+            
+            # print(">>>>>> train_loss <<<<<<", train_loss)
+            if epoch%10 == 0:
+                test_loss, label, expect_r = test_function(test_loader)
+                print(">>>>>> test_loss <<<<<< epoch", epoch , test_loss)
+                
+                if args.save_to_wandb:
+                    plt.plot(label[:500])
+                    plt.plot(expect_r[:500])
+                    plt.ylabel('r distance')
+                    plt.xlabel('number of test point')
+                    wandb.log({'distance': plt}, step=epoch)
+                    wandb.log({'Test_loss': test_loss}, step=epoch)
     
-
-    torch.save(model.state_dict(), os.path.join(wandb.run.dir, 'fir_6cov_1.pt'))
+    if args.save_to_wandb:
+        torch.save(model.state_dict(), os.path.join(wandb.run.dir, 'fir_6cov_1.pt'))
 
 
     
diff --git a/training_model/wandb/debug.log b/training_model/wandb/debug.log
index b536701..0fca9f5 100644
Binary files a/training_model/wandb/debug.log and b/training_model/wandb/debug.log differ
